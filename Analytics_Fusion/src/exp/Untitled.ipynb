{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca91c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m     top_recommendations\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_recommendations.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data(data_path)\n\u001b[1;32m     67\u001b[0m data, client_encoder, product_encoder \u001b[38;5;241m=\u001b[39m encode_ids(data)\n\u001b[0;32m---> 69\u001b[0m interaction_matrix \u001b[38;5;241m=\u001b[39m create_interaction_matrix(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClient_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonetary_Value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m interaction_matrix_values \u001b[38;5;241m=\u001b[39m interaction_matrix\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     72\u001b[0m num_clients, num_products \u001b[38;5;241m=\u001b[39m interaction_matrix\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mcreate_interaction_matrix\u001b[0;34m(data, client_col, product_col, rating_col)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_interaction_matrix\u001b[39m(data, client_col, product_col, rating_col):\n\u001b[0;32m---> 23\u001b[0m     interaction_matrix \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39mclient_col, columns\u001b[38;5;241m=\u001b[39mproduct_col, values\u001b[38;5;241m=\u001b[39mrating_col)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m interaction_matrix\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:8414\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[0;34m(self, columns, index, values)\u001b[0m\n\u001b[1;32m   8409\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8410\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   8411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, columns, index\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mNoDefault, values\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mNoDefault) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   8412\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[0;32m-> 8414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pivot(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns, values\u001b[38;5;241m=\u001b[39mvalues)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:557\u001b[0m, in \u001b[0;36mpivot\u001b[0;34m(data, columns, index, values)\u001b[0m\n\u001b[1;32m    553\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m result \u001b[38;5;241m=\u001b[39m indexed\u001b[38;5;241m.\u001b[39munstack(columns_listlike)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    558\u001b[0m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    559\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m    560\u001b[0m ]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4313\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[0;34m(self, level, fill_value)\u001b[0m\n\u001b[1;32m   4270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4271\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[1;32m   4272\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[1;32m   4310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4311\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[0;32m-> 4313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstack(\u001b[38;5;28mself\u001b[39m, level, fill_value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:488\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value)\n\u001b[0;32m--> 488\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m _Unstacker(\n\u001b[1;32m    489\u001b[0m     obj\u001b[38;5;241m.\u001b[39mindex, level\u001b[38;5;241m=\u001b[39mlevel, constructor\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor_expanddim\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[1;32m    492\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:136\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[0;34m(self, index, level, constructor)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[1;32m    129\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m         PerformanceWarning,\n\u001b[1;32m    133\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    134\u001b[0m     )\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_selectors()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:188\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[0;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import xgboost as xgb\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data['Brand_Name'].fillna('Unknown', inplace=True)\n",
    "    return data\n",
    "\n",
    "def encode_ids(data):\n",
    "    client_encoder = LabelEncoder()\n",
    "    product_encoder = LabelEncoder()\n",
    "\n",
    "    data['Client_ID'] = client_encoder.fit_transform(data['Client_ID'])\n",
    "    data['Product_ID'] = product_encoder.fit_transform(data['Product_ID'])\n",
    "    return data, client_encoder, product_encoder\n",
    "\n",
    "def create_interaction_matrix(data, client_col, product_col, rating_col):\n",
    "    interaction_matrix = data.pivot(index=client_col, columns=product_col, values=rating_col).fillna(0)\n",
    "    return interaction_matrix\n",
    "\n",
    "def build_matrix_factorization_model(num_clients, num_products, embedding_dim=50):\n",
    "    client_input = keras.layers.Input(shape=(1,), name='client_input')\n",
    "    client_embedding = keras.layers.Embedding(input_dim=num_clients, output_dim=embedding_dim, name='client_embedding')(client_input)\n",
    "    client_vec = keras.layers.Flatten()(client_embedding)\n",
    "\n",
    "    product_input = keras.layers.Input(shape=(1,), name='product_input')\n",
    "    product_embedding = keras.layers.Embedding(input_dim=num_products, output_dim=embedding_dim, name='product_embedding')(product_input)\n",
    "    product_vec = keras.layers.Flatten()(product_embedding)\n",
    "\n",
    "    dot_product = keras.layers.Dot(axes=1)([client_vec, product_vec])\n",
    "    output = keras.layers.Dense(1)(dot_product)\n",
    "\n",
    "    model = keras.models.Model(inputs=[client_input, product_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_matrix_factorization_model(model, X_train, y_train, epochs=5, batch_size=64, validation_split=0.2):\n",
    "    model.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "def generate_recommendations(client_encoder, specific_client_id, model, num_products):\n",
    "    specific_client_encoded_id = client_encoder.transform([specific_client_id])[0]\n",
    "    client_product_combinations = np.array([(specific_client_encoded_id, product) for product in range(num_products)])\n",
    "\n",
    "    all_client_ids = client_product_combinations[:, 0]\n",
    "    all_product_ids = client_product_combinations[:, 1]\n",
    "    predictions = model.predict([all_client_ids, all_product_ids]).flatten()\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "    predictions_normalized = scaler.fit_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Client_ID': [specific_client_id] * len(predictions_normalized),\n",
    "        'Product_ID': all_product_ids,\n",
    "        'Predicted_Rating': predictions_normalized\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    data_path = \"/Users/igormol/Desktop/analytics3/sample/Marajo.csv\"\n",
    "    specific_client_original_id = \"40.146.358 ADRIANA CAMPOS MATOS\"\n",
    "\n",
    "    data = load_data(data_path)\n",
    "    data, client_encoder, product_encoder = encode_ids(data)\n",
    "\n",
    "    interaction_matrix = create_interaction_matrix(data, 'Client_ID', 'Product_ID', 'Monetary_Value')\n",
    "    interaction_matrix_values = interaction_matrix.values\n",
    "\n",
    "    num_clients, num_products = interaction_matrix.shape\n",
    "\n",
    "    X = np.argwhere(interaction_matrix_values)\n",
    "    y = interaction_matrix_values[X[:, 0], X[:, 1]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    mf_model = build_matrix_factorization_model(num_clients, num_products)\n",
    "    train_matrix_factorization_model(mf_model, X_train, y_train)\n",
    "\n",
    "    all_predictions = generate_recommendations(client_encoder, specific_client_original_id, mf_model, num_products)\n",
    "    all_predictions = all_predictions.merge(data[['Product_ID', 'Product_Name']].drop_duplicates(), on='Product_ID', how='left')\n",
    "    all_predictions = all_predictions.sort_values(by='Predicted_Rating', ascending=False)\n",
    "\n",
    "    print(all_predictions)\n",
    "\n",
    "    # Save the top recommendations to a CSV file\n",
    "    top_recommendations = all_predictions.nlargest(10, 'Predicted_Rating')\n",
    "    top_recommendations.to_csv('top_recommendations.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2833a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "379/379 [==============================] - 6s 14ms/step - loss: 3007147.2500 - val_loss: 3406887.5000\n",
      "Epoch 2/5\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 3006798.0000 - val_loss: 3406531.5000\n",
      "Epoch 3/5\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 3005908.7500 - val_loss: 3405911.5000\n",
      "Epoch 4/5\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 3003254.7500 - val_loss: 3404748.0000\n",
      "Epoch 5/5\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 2997665.2500 - val_loss: 3402943.2500\n",
      "96/96 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Product_Name'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m     top_recommendations\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_recommendations.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m train_matrix_factorization_model(mf_model, X_train, y_train)\n\u001b[1;32m     86\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m generate_recommendations(client_encoder, specific_client_original_id, mf_model, num_products)\n\u001b[0;32m---> 87\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m all_predictions\u001b[38;5;241m.\u001b[39mmerge(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct_Name\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates(), on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     88\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m all_predictions\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_predictions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Product_Name'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import xgboost as xgb\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data['Brand_Name'].fillna('Unknown', inplace=True)\n",
    "    return data\n",
    "\n",
    "def encode_ids(data):\n",
    "    client_encoder = LabelEncoder()\n",
    "    product_encoder = LabelEncoder()\n",
    "\n",
    "    data['Client_ID'] = client_encoder.fit_transform(data['Client_ID'])\n",
    "    data['Product_ID'] = product_encoder.fit_transform(data['Product_ID'])\n",
    "    return data, client_encoder, product_encoder\n",
    "\n",
    "def aggregate_duplicates(data, client_col, product_col, rating_col):\n",
    "    return data.groupby([client_col, product_col], as_index=False)[rating_col].sum()\n",
    "\n",
    "def create_interaction_matrix(data, client_col, product_col, rating_col):\n",
    "    interaction_matrix = data.pivot(index=client_col, columns=product_col, values=rating_col).fillna(0)\n",
    "    return interaction_matrix\n",
    "\n",
    "def build_matrix_factorization_model(num_clients, num_products, embedding_dim=50):\n",
    "    client_input = keras.layers.Input(shape=(1,), name='client_input')\n",
    "    client_embedding = keras.layers.Embedding(input_dim=num_clients, output_dim=embedding_dim, name='client_embedding')(client_input)\n",
    "    client_vec = keras.layers.Flatten()(client_embedding)\n",
    "\n",
    "    product_input = keras.layers.Input(shape=(1,), name='product_input')\n",
    "    product_embedding = keras.layers.Embedding(input_dim=num_products, output_dim=embedding_dim, name='product_embedding')(product_input)\n",
    "    product_vec = keras.layers.Flatten()(product_embedding)\n",
    "\n",
    "    dot_product = keras.layers.Dot(axes=1)([client_vec, product_vec])\n",
    "    output = keras.layers.Dense(1)(dot_product)\n",
    "\n",
    "    model = keras.models.Model(inputs=[client_input, product_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_matrix_factorization_model(model, X_train, y_train, epochs=5, batch_size=64, validation_split=0.2):\n",
    "    model.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "def generate_recommendations(client_encoder, specific_client_id, model, num_products):\n",
    "    specific_client_encoded_id = client_encoder.transform([specific_client_id])[0]\n",
    "    client_product_combinations = np.array([(specific_client_encoded_id, product) for product in range(num_products)])\n",
    "\n",
    "    all_client_ids = client_product_combinations[:, 0]\n",
    "    all_product_ids = client_product_combinations[:, 1]\n",
    "    predictions = model.predict([all_client_ids, all_product_ids]).flatten()\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "    predictions_normalized = scaler.fit_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Client_ID': [specific_client_id] * len(predictions_normalized),\n",
    "        'Product_ID': all_product_ids,\n",
    "        'Predicted_Rating': predictions_normalized\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    data_path = \"/Users/igormol/Desktop/analytics3/sample/Marajo.csv\"\n",
    "    specific_client_original_id = \"40.146.358 ADRIANA CAMPOS MATOS\"\n",
    "\n",
    "    data = load_data(data_path)\n",
    "    data, client_encoder, product_encoder = encode_ids(data)\n",
    "    data = aggregate_duplicates(data, 'Client_ID', 'Product_ID', 'Monetary_Value')\n",
    "\n",
    "    interaction_matrix = create_interaction_matrix(data, 'Client_ID', 'Product_ID', 'Monetary_Value')\n",
    "    interaction_matrix_values = interaction_matrix.values\n",
    "\n",
    "    num_clients, num_products = interaction_matrix.shape\n",
    "\n",
    "    X = np.argwhere(interaction_matrix_values)\n",
    "    y = interaction_matrix_values[X[:, 0], X[:, 1]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    mf_model = build_matrix_factorization_model(num_clients, num_products)\n",
    "    train_matrix_factorization_model(mf_model, X_train, y_train)\n",
    "\n",
    "    all_predictions = generate_recommendations(client_encoder, specific_client_original_id, mf_model, num_products)\n",
    "    all_predictions = all_predictions.merge(data[['Product_ID', 'Product_Name']].drop_duplicates(), on='Product_ID', how='left')\n",
    "    all_predictions = all_predictions.sort_values(by='Predicted_Rating', ascending=False)\n",
    "\n",
    "    print(all_predictions)\n",
    "\n",
    "    # Save the top recommendations to a CSV file\n",
    "    top_recommendations = all_predictions.nlargest(10, 'Predicted_Rating')\n",
    "    top_recommendations.to_csv('top_recommendations.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8143b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "379/379 [==============================] - 8s 19ms/step - loss: 3007144.2500 - val_loss: 3406882.7500\n",
      "Epoch 2/5\n",
      "379/379 [==============================] - 7s 20ms/step - loss: 3006785.2500 - val_loss: 3406530.5000\n",
      "Epoch 3/5\n",
      "379/379 [==============================] - 6s 16ms/step - loss: 3005905.2500 - val_loss: 3405913.7500\n",
      "Epoch 4/5\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 3003214.7500 - val_loss: 3404727.0000\n",
      "Epoch 5/5\n",
      "379/379 [==============================] - 6s 17ms/step - loss: 2997497.5000 - val_loss: 3402864.5000\n",
      "96/96 [==============================] - 0s 3ms/step\n",
      "                            Client_ID  Product_ID  Predicted_Rating  \\\n",
      "305   40.146.358 ADRIANA CAMPOS MATOS         305        100.000000   \n",
      "66    40.146.358 ADRIANA CAMPOS MATOS          66         62.130764   \n",
      "13    40.146.358 ADRIANA CAMPOS MATOS          13         57.482574   \n",
      "848   40.146.358 ADRIANA CAMPOS MATOS         848         55.076477   \n",
      "2567  40.146.358 ADRIANA CAMPOS MATOS        2567         50.433125   \n",
      "...                               ...         ...               ...   \n",
      "1808  40.146.358 ADRIANA CAMPOS MATOS        1808         16.929913   \n",
      "820   40.146.358 ADRIANA CAMPOS MATOS         820         16.690937   \n",
      "331   40.146.358 ADRIANA CAMPOS MATOS         331         15.723106   \n",
      "308   40.146.358 ADRIANA CAMPOS MATOS         308         13.520274   \n",
      "306   40.146.358 ADRIANA CAMPOS MATOS         306          0.000000   \n",
      "\n",
      "                        Product_Name  \n",
      "305   GELADEIRA EXTERNA UNIVERSAL 67  \n",
      "66                     ARLA 32 20LTS  \n",
      "13      SHELL SPIRAX S1 ATF TASA 1LT  \n",
      "848   GELADEIRA CAB41 12/24VCC ELBER  \n",
      "2567     AIR LIFE LAVANDA 530ML XCAR  \n",
      "...                              ...  \n",
      "1808      LAVADORA PETIT BRANCA 220V  \n",
      "820               BATERIA M180BD MFA  \n",
      "331              GELADEIRA RES18L BV  \n",
      "308   GELADEIRA PORTATIL UNIV 31L BV  \n",
      "306   GELADEIRA PORT.UNIV.31L QUADRI  \n",
      "\n",
      "[3056 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import xgboost as xgb\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data['Brand_Name'].fillna('Unknown', inplace=True)\n",
    "    return data\n",
    "\n",
    "def encode_ids(data):\n",
    "    client_encoder = LabelEncoder()\n",
    "    product_encoder = LabelEncoder()\n",
    "\n",
    "    data['Client_ID'] = client_encoder.fit_transform(data['Client_ID'])\n",
    "    data['Product_ID'] = product_encoder.fit_transform(data['Product_ID'])\n",
    "    return data, client_encoder, product_encoder\n",
    "\n",
    "def aggregate_duplicates(data, client_col, product_col, rating_col):\n",
    "    aggregated_data = data.groupby([client_col, product_col], as_index=False)[rating_col].sum()\n",
    "    # Keep the first occurrence of Product_Name for each Product_ID\n",
    "    aggregated_data = aggregated_data.merge(data[['Product_ID', 'Product_Name']].drop_duplicates(), on=product_col, how='left')\n",
    "    return aggregated_data\n",
    "\n",
    "def create_interaction_matrix(data, client_col, product_col, rating_col):\n",
    "    interaction_matrix = data.pivot(index=client_col, columns=product_col, values=rating_col).fillna(0)\n",
    "    return interaction_matrix\n",
    "\n",
    "def build_matrix_factorization_model(num_clients, num_products, embedding_dim=50):\n",
    "    client_input = keras.layers.Input(shape=(1,), name='client_input')\n",
    "    client_embedding = keras.layers.Embedding(input_dim=num_clients, output_dim=embedding_dim, name='client_embedding')(client_input)\n",
    "    client_vec = keras.layers.Flatten()(client_embedding)\n",
    "\n",
    "    product_input = keras.layers.Input(shape=(1,), name='product_input')\n",
    "    product_embedding = keras.layers.Embedding(input_dim=num_products, output_dim=embedding_dim, name='product_embedding')(product_input)\n",
    "    product_vec = keras.layers.Flatten()(product_embedding)\n",
    "\n",
    "    dot_product = keras.layers.Dot(axes=1)([client_vec, product_vec])\n",
    "    output = keras.layers.Dense(1)(dot_product)\n",
    "\n",
    "    model = keras.models.Model(inputs=[client_input, product_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_matrix_factorization_model(model, X_train, y_train, epochs=5, batch_size=64, validation_split=0.2):\n",
    "    model.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "def generate_recommendations(client_encoder, specific_client_id, model, num_products):\n",
    "    specific_client_encoded_id = client_encoder.transform([specific_client_id])[0]\n",
    "    client_product_combinations = np.array([(specific_client_encoded_id, product) for product in range(num_products)])\n",
    "\n",
    "    all_client_ids = client_product_combinations[:, 0]\n",
    "    all_product_ids = client_product_combinations[:, 1]\n",
    "    predictions = model.predict([all_client_ids, all_product_ids]).flatten()\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "    predictions_normalized = scaler.fit_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Client_ID': [specific_client_id] * len(predictions_normalized),\n",
    "        'Product_ID': all_product_ids,\n",
    "        'Predicted_Rating': predictions_normalized\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    data_path = \"/Users/igormol/Desktop/analytics3/sample/Marajo.csv\"\n",
    "    specific_client_original_id = \"40.146.358 ADRIANA CAMPOS MATOS\"\n",
    "\n",
    "    data = load_data(data_path)\n",
    "    data, client_encoder, product_encoder = encode_ids(data)\n",
    "    data = aggregate_duplicates(data, 'Client_ID', 'Product_ID', 'Monetary_Value')\n",
    "\n",
    "    interaction_matrix = create_interaction_matrix(data, 'Client_ID', 'Product_ID', 'Monetary_Value')\n",
    "    interaction_matrix_values = interaction_matrix.values\n",
    "\n",
    "    num_clients, num_products = interaction_matrix.shape\n",
    "\n",
    "    X = np.argwhere(interaction_matrix_values)\n",
    "    y = interaction_matrix_values[X[:, 0], X[:, 1]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    mf_model = build_matrix_factorization_model(num_clients, num_products)\n",
    "    train_matrix_factorization_model(mf_model, X_train, y_train)\n",
    "\n",
    "    all_predictions = generate_recommendations(client_encoder, specific_client_original_id, mf_model, num_products)\n",
    "    all_predictions = all_predictions.merge(data[['Product_ID', 'Product_Name']].drop_duplicates(), on='Product_ID', how='left')\n",
    "    all_predictions = all_predictions.sort_values(by='Predicted_Rating', ascending=False)\n",
    "\n",
    "    print(all_predictions)\n",
    "\n",
    "    # Save the top recommendations to a CSV file\n",
    "    top_recommendations = all_predictions.nlargest(10, 'Predicted_Rating')\n",
    "    top_recommendations.to_csv('top_recommendations.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4b765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
